---
title: "回归方法的有效性"
subtitle: ""
author: "冯凌秉"
institute: "<span style = 'font-size: 70%;'>
  江西财经大学 <br> 
  产业经济研究院</span>"
date: '2020<br><br>
  `r icon::fa("paper-plane")` <feng.lingbing@jxufe.edu.cn>'
output:
  xaringan::moon_reader:
    css: [default, zh-CN.css]
    lib_dir: libs
    includes:
      after_body: insert-logo.html
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---
```{r, setup, include=FALSE,echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = ">", 
                      fig.retina = 3, warning = FALSE, message = FALSE)
library(RefManageR)
library(AER)
library(stargazer)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
BibOptions(check.entries = FALSE, bib.style = "authoryear", style = "markdown", dashed = TRUE)
bib <- ReadBib("bib1.bib")
css.fig <- list(
    css.depvarhead = 'color: red;',
    css.centeralign = 'text-align: left;', 
    css.firsttablecol = 'font-weight: bold;', 
    css.summary = 'color: blue;'
  )
```

# 回归方法的有效性

- Internal validity：关于因果效应的推断对考虑对象的总体依然有效，则称这个方法是内部有效的。

- External validity：关于因果效应的推断不仅对考察对象的总体有效，而且可以泛化到其他的总体或者环境，则称为外部有效。

内部有效的两个条件：
1. 因果关系的估计量的测度，也就是回归变量的系数估计，应该是无偏和一致的。

2. 系数估计的标准误应该是有效的。

外部有效是很难满足的，发生如下情况的任何一种，都难以保证外部有效性:

1. 外部的总体和回归研究的总体不同.

2. 外部总体的某个条件/环境/设置与回归研究总体的条件/环境/设置不同。

---
# 内部有效性与函数设定错误

.pull-left[
### 内部有效性的威胁

1. 缺失变量偏差 (OVB)
2. 函数形式设定错误
3. 测量误差 (measurement error)
4. 缺失值与样本选择偏差 (sample selection bias)
5. 逆向因果 (reverse causality)

这五种情况都会导致一个关键的OLS假设不成立:
$$E(u_i\vert X_{1i},\dots ,X_{ki}) \neq 0$$
继而导致OLS估计量的无偏性和一致性受损。

函数设定错误的典型情形：用线性函数拟合明显的非线性关系。
]

.pull-right[
```{r echo=F}
set.seed(3)
X <- runif(100, -5, 5)
Y <- X^2 + rnorm(100)
ms_mod <- lm(Y ~ X)
ms_mod2 <- lm(Y~X + I(X^2))
plot(X, Y, 
     main = "Misspecification of Functional Form",
     pch = 20,
     col = "steelblue")
abline(ms_mod, 
       col = "darkred",
       lwd = 2)
```

]

---
# 测量误差
对于某关键变量，如果我们观测到的 $\overset{\sim}{X}_i$ 不是准确的测量值，存在测量上的误差，那么总体的回归模型为：
$$Y_i = \beta_0 + \beta_1 X_i + u_i$$

而我们估计的模型为
$$\begin{align*}
  Y_i =& \, \beta_0 + \beta_1 \overset{\sim}{X}_i + \underbrace{\beta_1 (X_i - \overset{\sim}{X}_i) + u_i}_{=v_i} \\
  Y_i =& \, \beta_0 + \beta_1 \overset{\sim}{X}_i + v_i
\end{align*}$$
明显的, $\overset{\sim}{X}_i$ 与此时的误差项 $v_i$ 是相关的。此时OLS估计将失去无偏性和一致性。

### [思考]：在什么情况下会发生较为严重的测量误差？

---
假设测量误差满足
$$\begin{equation}
  \overset{\sim}{X}_i = X_i + w_i, \ \ \rho_{w_i,u_i}=0, \ \ \rho_{w_i,X_i}=0 
\end{equation}$$
那么
$$\begin{equation}
  \widehat{\beta}_1 \xrightarrow{p}{\frac{\sigma_{X}^2}{\sigma_{X}^2 + \sigma_{w}^2}} \beta_1
\end{equation}$$

假设模拟：
$$\begin{align*}
  Y_i =& \, 100 + 0.5 (X_i - 50) \\
      =& \, 75 + 0.5 X_i. 
\end{align*}$$
X的测量存在误差 $\overset{\sim}{X_i} = X_i + w_i$，其中 $w_i \overset{i.i.d.}{\sim} \mathcal{N}(0,10)$。

.pull-left[
```{r eval=F}
set.seed(1)
library(mvtnorm)
dat <- data.frame(
  rmvnorm(1000, c(50, 100), 
          sigma = cbind(c(10, 5), c(5, 10))))
colnames(dat) <- c("X", "Y")
noerror_mod <- lm(Y ~ X, data = dat)
dat$X <- dat$X + rnorm(n = 1000, sd = sqrt(10))
error_mod <- lm(Y ~ X, data = dat)
noerror_mod$coefficients
```
]
.pull-right[
X的系数估计明显“低估”了真实的线性关系强度。

```{r echo=F}
set.seed(1)
library(mvtnorm)
dat <- data.frame(
  rmvnorm(1000, c(50, 100), 
          sigma = cbind(c(10, 5), c(5, 10))))
colnames(dat) <- c("X", "Y")
noerror_mod <- lm(Y ~ X, data = dat)
dat$X <- dat$X + rnorm(n = 1000, sd = sqrt(10))
error_mod <- lm(Y ~ X, data = dat)
noerror_mod$coefficients
error_mod$coefficients
```
]

---
# 系数纠正
.pull-left[
```{r plot_measure_error, fig.show='hide'}
plot(dat$X, dat$Y, 
     pch = 20, 
     col = "steelblue",
     xlab = "X",
     ylab = "Y")
abline(coef = c(75, 0.5), 
       col = "darkgreen",
       lwd  = 1.5)
abline(noerror_mod, 
       col = "purple",
       lwd  = 1.5)
abline(error_mod, 
       col = "darkred",
       lwd  = 1.5)
legend("topleft",
       bg = "transparent",
       cex = 0.8,
       lty = 1,
       col = c("darkgreen", "purple", "darkred"), 
       legend = c("Population", "No Errors", "Errors"))    
```
]

.pull-right[
$$\frac{\sigma_X^2 + \sigma_w^2}{\sigma_X^2} \cdot \widehat{\beta}_1 = \frac{10+10}{10} \cdot 0.255 = 0.51$$
```{r ref.label = 'plot_measure_error', echo=F, fig.height=6}
plot(dat$X, dat$Y, 
     pch = 20, 
     col = "steelblue",
     xlab = "X",
     ylab = "Y")
abline(coef = c(75, 0.5), 
       col = "darkgreen",
       lwd  = 1.5)
abline(noerror_mod, 
       col = "purple",
       lwd  = 1.5)

abline(error_mod, 
       col = "darkred",
       lwd  = 1.5)
legend("topleft",
       bg = "transparent",
       cex = 0.8,
       lty = 1,
       col = c("darkgreen", "purple", "darkred"), 
       legend = c("Population", "No Errors", "Errors"))
```
]

---
# 数据缺失与样本选择偏差


.pull-left[
数据缺失可能有三种情形
第一种情况（完全随机缺失）不会损害OLS估计的无偏性，但是会降低有效性，相当于减少了样本量。
```{r plot_missing_MCAR, fig.show='hide'}
set.seed(1)
dat <- data.frame(
  rmvnorm(1000, c(50, 100), 
          sigma = cbind(c(10, 5), c(5, 10))))
colnames(dat) <- c("X", "Y")
id <- sample(1:1000, size = 500)
plot(dat$X[-id], dat$Y[-id], col = "steelblue", pch = 20,
     cex = 0.8,xlab = "X", ylab = "Y")
points(dat$X[id], dat$Y[id],cex = 0.8,
       col = "gray", pch = 20)
abline(coef = c(75, 0.5), col = "darkgreen",lwd  = 1.5)
abline(noerror_mod)
dat <- dat[-id, ]
c1_mod <- lm(dat$Y ~ dat$X, data = dat)
abline(c1_mod, col = "purple")
legend("topleft",lty = 1,bg = "transparent",
       cex = 0.8,col = c("darkgreen", "black", "purple"), 
       legend = c("Population", "Full sample", "500 obs. randomly selected"))    
```
]

.pull-right[
```{r ref.label = 'plot_missing_MCAR', echo=F, fig.height=7}
```
]

---
# 缺失与X有关
.pull-left[
假设缺失机制与某个X变量有关，比如所有 $X>45$ 的观测缺失，这样也不会影响OLS估计的无偏性，同样是降低了估计的有效性。
```{r plot_missing_MAR, fig.show='hide'}
set.seed(123)
dat <- data.frame(
  rmvnorm(1000, c(50, 100), 
          sigma = cbind(c(10, 5), c(5, 10))))
colnames(dat) <- c("X", "Y")
id <- dat$X >= 45
plot(dat$X[-id], dat$Y[-id], col = "steelblue",
     cex = 0.8,pch = 20,xlab = "X",ylab = "Y")
points(dat$X[id], dat$Y[id],  col = "gray",
       cex = 0.8,pch = 20)
abline(coef = c(75, 0.5), col = "darkgreen",lwd  = 1.5)
abline(noerror_mod)
dat <- dat[!id, ]
c2_mod <- lm(dat$Y ~ dat$X, data = dat)
abline(c2_mod, col = "purple")
legend("topleft",lty = 1,bg = "transparent",
       cex = 0.8,col = c("darkgreen", "black", "purple"), 
       legend = c("Population", "Full sample", "Obs. with X <= 45"))
```
]

.pull-right[
```{r ref.label = 'plot_missing_MAR', echo=F, fig.height=7}
```
]

---
# 缺失与Y有关
.pull-left[

```{r plot_missing_MN, fig.show='hide'}
set.seed(1)
dat <- data.frame(
  rmvnorm(1000, c(50,100), 
          sigma = cbind(c(10,5), c(5,10))))
colnames(dat) <- c("X","Y")
id <- which(dat$X <= 55 & dat$Y >= 100)
plot(dat$X[-id], dat$Y[-id], col = "gray",
     cex = 0.8,pch = 20,xlab = "X",ylab = "Y")
points(dat$X[id], dat$Y[id], col = "steelblue",
     cex = 0.8,pch = 20)
abline(coef = c(75, 0.5), col = "darkgreen",lwd  = 1.5)
abline(noerror_mod)
dat <- dat[id, ]
c3_mod <- lm(dat$Y ~ dat$X, data = dat)
abline(c3_mod, col = "purple")
legend("topleft",lty = 1,bg = "transparent",
       cex = 0.8,col = c("darkgreen", "black", "purple"), 
       legend = c("Population", "Full sample", "X <= 55 & Y >= 100"))
```
]

.pull-right[
缺失的数据为 $X_i < 55,Y_i>100$, 这种情况会导致OLS的估计有偏。
```{r ref.label = 'plot_missing_MN', echo=F, fig.height=7}
```
]

---
# 逆向因果
在通常的回归关系中，我们假设X的变化可以导致Y的变化，因此因果关系的方向是从X到Y。如果Y的变化也可以导致X的变化，则称为存在逆向因果(reverse causality)关系，或者叫做同时因果关系 (simultaneous causality)。

逆向因果会导致X与误差项相关，从而导致OLS估计出现偏误。
```{r}
library(AER)
data("CigarettesSW")
c1995 <- subset(CigarettesSW, year == "1995")
cigcon_mod <- lm(log(packs) ~ log(price), data = c1995)
cigcon_mod
```

---
# 实例

.pull-left[
```{r plot_exterval_val, fig.show='hide'}
library(AER)
data(MASchools)
data("CASchools")
MASchools$score <- MASchools$score4 
MASchools$STR <- MASchools$stratio
Linear_model_MA <- lm(score ~ income, data = MASchools)
Linearlog_model_MA <- lm(score ~ log(income), data = MASchools) 
cubic_model_MA <- lm(score ~ I(income) + I(income^2) + I(income^3), data = MASchools)
plot(MASchools$income, MASchools$score,
     pch = 20,
     col = "steelblue",
     xlab = "District income",
     ylab = "Test score",
     xlim = c(0, 50),
     ylim = c(620, 780))
abline(Linear_model_MA, lwd = 2)
order_id  <- order(MASchools$income)
lines(MASchools$income[order_id],
      fitted(Linearlog_model_MA)[order_id], 
      col = "darkgreen", 
      lwd = 2)
lines(x = MASchools$income[order_id], 
      y = fitted(cubic_model_MA)[order_id],
      col = "orange", 
      lwd = 2) 
legend("topleft",
       legend = c("Linear", "Linear-Log", "Cubic"),
       lty = 1,
       col = c("Black", "darkgreen", "orange"))    
```
]

.pull-right[
```{r ref.label = 'plot_exterval_val', echo=F, fig.height=7}
```
]

---
# 模型比较
```{r}
MASchools$HiEL <- as.numeric(MASchools$english > median(MASchools$english))
m1 <- lm(score ~ STR, data = MASchools)

m2 <- lm(score ~ STR + english + lunch + log(income), 
                        data = MASchools)
m3 <- lm(score ~ STR + english + lunch + income + I(income^2) 
                        + I(income^3), data = MASchools)
m4 <- lm(score ~ STR + I(STR^2) + I(STR^3) + english + lunch + income
                        + I(income^2) + I(income^3), data = MASchools)
m5 <- lm(score ~ STR + I(income^2) + I(income^3) + HiEL:STR + lunch 
                        + income, data = MASchools)
m6 <- lm(score ~ STR + I(income^2) + I(income^3) + HiEL + HiEL:STR + lunch + income, data = MASchools)
```
---


```{r echo=F}
tab_model(m1, m2, m3, m4, m5, m6, emph.p = T, robust = T,
          show.ci = F, collapse.se = T, dv.labels = paste0("M", 1:6),string.pred = "Coeffcient", show.se = F,
          p.style = "a")
```

---
# 非线性检验
```{r}
linearHypothesis(m3, c("I(income^2)=0", "I(income^3)=0"), 
                 vcov. = vcovHC, type = "HC1")$`Pr(>F)`[2]
linearHypothesis(m4,  c("STR=0", "I(STR^2)=0", "I(STR^3)=0"), 
                 vcov. = vcovHC, type = "HC1")$`Pr(>F)`[2]
linearHypothesis(m4,  c("I(STR^2)=0", "I(STR^3)=0"), 
                 vcov. = vcovHC, type = "HC1")$`Pr(>F)`[2]
linearHypothesis(m5, c("STR=0", "STR:HiEL=0"), 
                 vcov. = vcovHC, type = "HC1")$`Pr(>F)`[2]
linearHypothesis(m5, c("I(income^2)=0", "I(income^3)=0"), 
                 vcov. = vcovHC, type = "HC1")$`Pr(>F)`[2]
linearHypothesis(m6,c("HiEL=0", "STR:HiEL=0"), 
                 vcov. = vcovHC, type = "HC1")$`Pr(>F)`[2]
linearHypothesis(m6, 
                 c("I(income^2)=0", "I(income^3)=0"), 
                 vcov. = vcovHC, type = "HC1")$`Pr(>F)`[2]
```


